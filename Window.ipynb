{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-H-515 Project <br>\n",
    "2022â€“2023\n",
    "\n",
    "# Phase 1 : Sliding Window Processing\n",
    "Dimitris Sacharidis, Antonios Kontaxakis <br>\n",
    "EPB, ULB "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "Group Number : 5 <br>\n",
    "Group Members : Rania Baguia (000459242), Hakim Amri (000459153), Julian Cailliau (000459856), Mehdi Jdaoudi (000457507)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, rank, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.streaming import StreamingContext\n",
    "import socket\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import re, ast\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 19:06:03 WARN Utils: Your hostname, Mehdis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.205 instead (on interface en0)\n",
      "23/05/14 19:06:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/14 19:06:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/14 19:06:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .appName(\"Windowing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Let us retrieve the sparkContext object\n",
    "sc=spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Stream function\n",
    "This function will create a streaming context from a network socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDStream(sc, batch_interval):\n",
    "    \"\"\"\n",
    "    Create a streaming context and a DStream from a network socket.\n",
    "\n",
    "    Args:\n",
    "        sc (SparkContext): The Spark context object.\n",
    "        batch_interval (int): The time interval in seconds at which streaming data will be divided into batches.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the streaming context (ssc) and the DStream (dstream).\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> sc = SparkContext(appName=\"StreamingExample\")\n",
    "        >>> ssc, dstream = getDStream(sc, 5)\n",
    "    \"\"\"\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    # Create a DStream that represents streaming data from a network socket\n",
    "    dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "    \n",
    "    return [ssc,dstream]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, as we are computing the correlation during the window, there is no states needed. The formula for the pearson coefficient is the same as the stream processing question.\n",
    "\\begin{equation*}\n",
    "r_{xy} = \\dfrac{n\\sum x_i y_i - \\sum x_i \\sum y_i }{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnning the stream\n",
    "The following cells will run the top-5 computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS_LENGTH = 40\n",
    "SLIDE = 10\n",
    "BATCH_INTERVAL = 10\n",
    "N_SENSORS = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the DStream object containing the streaming data sent by the producer notebook\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataInWindow = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "    .window(WINDOWS_LENGTH, SLIDE)\n",
    "\n",
    "# Group the data by sensor\n",
    "dataInWindowPerSensor = dataInWindow\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .groupByKey()\n",
    "\n",
    "# Computing the cumulative sum\n",
    "cumSums = dataInWindowPerSensor\\\n",
    "    .mapValues(lambda x : sum([i[2] for i in x]))\\\n",
    "    .map(lambda x :(x[0], x[1]))\\\n",
    "\n",
    "# Computing the squared cumulative sum\n",
    "cumSumsSquared = dataInWindowPerSensor\\\n",
    "    .mapValues(lambda x : sum([i[2]**2 for i in x]))\\\n",
    "    .map(lambda x :(x[0], x[1]))\\\n",
    "\n",
    "def compute_covariance_at_T(x, indexes):\n",
    "    \"\"\"\n",
    "    Computes the covariance at time T for a given set of indexes.\n",
    "\n",
    "    Args:\n",
    "        x: The input data.\n",
    "        indexes: The indexes indicating which covariances to compute.\n",
    "\n",
    "    Returns:\n",
    "        The computed covariances at time T.\n",
    "    \"\"\"\n",
    "    x = sorted(x, key = lambda y : y[4])\n",
    "    covAtT = [[(row[4] + \"-\" + col[4], row[2]*col[2]) for col in x] for row in x]\n",
    "    covAtT = [covAtT[i[0]][i[1]] for i in indexes]\n",
    "    return covAtT\n",
    "\n",
    "# Computing the indexes for the covariance matrix\n",
    "# Grouping the records per timestamp and computing the covariance matrix at time T\n",
    "indexes = [(i, j) for i in range(N_SENSORS) for j in range(N_SENSORS) if j > i]\n",
    "Cum_Covariances = dataInWindow\\\n",
    "    .map(lambda x: (x[5], x))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda x : compute_covariance_at_T(x, indexes))\\\n",
    "    .flatMap(lambda x : [record for record in x[1]])\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda x : sum(x))\n",
    "\n",
    "# Obtaining the number of observations \n",
    "N = dataInWindow\\\n",
    "    .map(lambda x:1)\\\n",
    "    .reduce(lambda x,y:x+y)\\\n",
    "    .map(lambda x : x/N_SENSORS)\n",
    "\n",
    "def get_correlation(x):\n",
    "    \"\"\"\n",
    "    Computes the correlation using the provided formula.\n",
    "\n",
    "    Args:\n",
    "        x: The input data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the pair identifier and the computed correlation.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        corr = (x[7]*x[2]-x[3]*x[5])/((math.sqrt(x[7]*x[4]-(x[3]**2)))*(math.sqrt(x[7]*x[6]-(x[5]**2))))\n",
    "    except ZeroDivisionError:\n",
    "        corr = -np.inf\n",
    "    return (x[0] + \"-\" + x[1], corr)\n",
    "\n",
    "# Joining the streams to compute the correlation\n",
    "Correlation = Cum_Covariances\\\n",
    "    .map(lambda x : (*x[0].split(\"-\"), x[1]))\\\n",
    "    .map(lambda x : (x[0], x))\\\n",
    "    .join(cumSums)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .join(cumSumsSquared)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .map(lambda x : (x[1][1], x[1]))\\\n",
    "    .join(cumSums)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .join(cumSumsSquared)\\\n",
    "    .map(lambda x : (*x[1][0], x[1][1]))\\\n",
    "    .transformWith(lambda rdd1, rdd2: rdd1.cartesian(rdd2), N)\\\n",
    "    .map(lambda x : (*x[0], x[1]))\\\n",
    "    .map(get_correlation)\n",
    "\n",
    "# Filtering to get the top 5 correlations\n",
    "Top_5_per_Window = Correlation\\\n",
    "    .transform(lambda rdd:rdd.ctx.parallelize(rdd.takeOrdered(5, lambda x: -x[1])))\n",
    "\n",
    "#Top_5_per_Window.pprint()\n",
    "Top_5_per_Window.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 19:06:20\n",
      "-------------------------------------------\n",
      "('CB02411-CEK049', 0.7657636106436597)\n",
      "('CEK049-CJM90', 0.7500074623328697)\n",
      "('CB02411-CJM90', 0.7341503240487527)\n",
      "('CB2105-CJM90', 0.7071314753542198)\n",
      "('CB2105-CEK049', 0.6943233481656088)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 19:06:30\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8895591661284046)\n",
      "('CB02411-CEK049', 0.8806025427740716)\n",
      "('CB02411-CJM90', 0.8799457993377243)\n",
      "('CB1143-CB2105', 0.7056776812543091)\n",
      "('CB1143-CJM90', 0.6933695688222551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>    (0 + 1) / 1][Stage 26:=>  (1 + 1) / 4][Stage 28:>   (0 + 0) / 4]1]\r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 19:06:44 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "[Stage 30:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 19:06:40\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.819021704421524)\n",
      "('CB02411-CJM90', 0.7778404487961231)\n",
      "('CB02411-CEK049', 0.7759358889102512)\n",
      "('CB1143-CEK049', 0.7043333216786912)\n",
      "('CB1599-CLW239', 0.6986806136245722)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
