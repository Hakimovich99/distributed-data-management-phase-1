{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-H-515 Project <br>\n",
    "2022â€“2023\n",
    "\n",
    "# Phase 1 : Stream Processing - Consummer\n",
    "Dimitris Sacharidis, Antonios Kontaxakis <br>\n",
    "EPB, ULB "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "Group Number : 5 <br>\n",
    "Group Members : Rania Baguia (000459242), Hakim Amri (000459153), Julian Cailliau (000459856), Mehdi Jdaoudi (000457507)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, rank, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re, ast\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 18:39:43 WARN Utils: Your hostname, Mehdis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.205 instead (on interface en0)\n",
      "23/05/14 18:39:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/14 18:39:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/14 18:39:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[10]\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"10\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .appName(\"Consummer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Let us retrieve the sparkContext object\n",
    "sc=spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the necessary information\n",
    "We first load the sensors information, essentially sensor names. This allows us to programmatically create states for the following models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = []\n",
    "with open(\"data/bikes_sensors.json\", \"r\") as f:\n",
    "    sensors = json.load(f)\n",
    "    sensors = [\n",
    "        sensor[\"properties\"][\"device_name\"] for sensor in sensors[\"features\"]\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Stream function\n",
    "This function will create a streaming context from a network socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDStream(sc, batch_interval):\n",
    "    \"\"\"\n",
    "    Create a streaming context and a DStream from a network socket.\n",
    "\n",
    "    Args:\n",
    "        sc (SparkContext): The Spark context object.\n",
    "        batch_interval (int): The time interval in seconds at which streaming data will be divided into batches.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the streaming context (ssc) and the DStream (dstream).\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> sc = SparkContext(appName=\"StreamingExample\")\n",
    "        >>> ssc, dstream = getDStream(sc, 5)\n",
    "    \"\"\"\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    # Create a DStream that represents streaming data from a network socket\n",
    "    dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "    \n",
    "    return [ssc,dstream]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the states and update functions\n",
    "We are storing the state per sensor which correspond to the cumulative sums, state_per_sensor_squared which is the sum of the squares, the timestamp (the reason why in the preprocessing we added a timestamp) which is the length of the observations, and finally the cumulated sum per covariate. The following equation is used as it allows a single pass on the data, which is in line with streaming principles :\n",
    "\\begin{equation*}\n",
    "r_{xy} = \\dfrac{n\\sum x_i y_i - \\sum x_i \\sum y_i }{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_per_sensor = {i:0 for i in sensors}\n",
    "state_per_sensor_rdd = sc.parallelize([('state_per_sensor', state_per_sensor)])\n",
    "\n",
    "state_per_sensor_squared = {i:0 for i in sensors}\n",
    "state_per_sensor_squared_rdd = sc.parallelize([('state_per_sensor_squared', state_per_sensor_squared)])\n",
    "\n",
    "state_timestamp = 0\n",
    "state_timestamp_rdd = sc.parallelize([('state_timestamp', state_timestamp)])\n",
    "\n",
    "N_SENSORS = len(sensors)\n",
    "indexes = [(i, j) for i in range(N_SENSORS) for j in range(N_SENSORS) if j > i]\n",
    "pais_duplicates = [[row + \"-\" + col for col in sorted(sensors)] for row in sorted(sensors)]\n",
    "pairs = [pais_duplicates[i[0]][i[1]] for i in indexes]\n",
    "state_covariance_pairs = {i:0 for i in pairs}\n",
    "state_covariance_pairs_rdd = sc.parallelize([('state_covariance_pairs', state_covariance_pairs)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the states structure, the same update function can be used all the time, as it is just a dictionnary with k, the sensor, and v, the values. The only different update function is for the timestamp, as it is a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    \"\"\"\n",
    "    Updates the state with new values.\n",
    "\n",
    "    Args:\n",
    "        new_values: The new values to be incorporated into the state.\n",
    "        state: The current state.\n",
    "\n",
    "    Returns:\n",
    "        The updated state.\n",
    "    \"\"\"\n",
    "    L=len(new_values) \n",
    "    if (L>0):\n",
    "        initial_state = state\n",
    "        for l in np.arange(L):\n",
    "            sensor = new_values[l][0]\n",
    "            count = new_values[l][1]\n",
    "            initial_state[sensor] = initial_state[sensor] + count\n",
    "        return initial_state    \n",
    "    else:\n",
    "        return state\n",
    "\n",
    "def updateFunctionTimeStamp(new_values, state):\n",
    "    \"\"\"\n",
    "    Updates the state with new timestamped values.\n",
    "\n",
    "    Args:\n",
    "        new_values: The new timestamped values to be incorporated into the state.\n",
    "        state: The current state.\n",
    "\n",
    "    Returns:\n",
    "        The updated state.\n",
    "    \"\"\" \n",
    "    L=len(new_values) \n",
    "    if (L>0):\n",
    "        return new_values[0]\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnning the stream\n",
    "The following cells will run the top-5 computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 5\n",
    "N_SENSORS = len(sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the DStream object containing the streaming data sent by the producer notebook\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream.flatMap(lambda x: [*np.array(ast.literal_eval(x))])\n",
    "\n",
    "# Obtaining the last timestamp\n",
    "TimeStamp = dstream\\\n",
    "    .flatMap(lambda x: [(\"state_timestamp\", max([int(i[5]) for i in np.array(ast.literal_eval(x))]))])\n",
    "\n",
    "# Mapping the stream to its proper format\n",
    "dataS = dataS\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\n",
    "\n",
    "# Group the data by sensor\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .groupByKey()\\\n",
    "\n",
    "# Computing the cumulative sum\n",
    "cumSums = dataPerSensor\\\n",
    "    .mapValues(lambda x : sum([i[2] for i in x]))\\\n",
    "    .map(lambda x :(x[0], x[1]))\\\n",
    "    .flatMap(lambda x : [(\"state_per_sensor\", x)])\n",
    "\n",
    "# Computing the squared cumulative sum\n",
    "cumSums_squared = dataPerSensor\\\n",
    "    .mapValues(lambda x : sum([i[2]**2 for i in x]))\\\n",
    "    .map(lambda x :(x[0], x[1]))\\\n",
    "    .flatMap(lambda x : [(\"state_per_sensor_squared\", x)])\n",
    "\n",
    "def compute_covariance_at_T(x, indexes):\n",
    "    \"\"\"\n",
    "    Computes the covariance at time T for a given set of indexes.\n",
    "\n",
    "    Args:\n",
    "        x: The input data.\n",
    "        indexes: The indexes indicating which covariances to compute.\n",
    "\n",
    "    Returns:\n",
    "        The computed covariances at time T.\n",
    "    \"\"\"\n",
    "    x = sorted(x, key = lambda y : y[4])\n",
    "    covAtT = [[(row[4] + \"-\" + col[4], row[2]*col[2]) for col in x] for row in x]\n",
    "    covAtT = [covAtT[i[0]][i[1]] for i in indexes]\n",
    "    return covAtT\n",
    "\n",
    "\n",
    "# Computing the indexes for the covariance matrix\n",
    "# Grouping the records per timestamp and computing the covariance matrix at time T\n",
    "indexes = [(i, j) for i in range(N_SENSORS) for j in range(N_SENSORS) if j > i]\n",
    "Cum_Covariances = dataS\\\n",
    "    .map(lambda x: (x[5], x))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda x : compute_covariance_at_T(x, indexes))\\\n",
    "    .flatMap(lambda x : [record for record in x[1]])\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda x : sum(x))\\\n",
    "    .flatMap(lambda x : [(\"state_covariance_pairs\", x)])\n",
    "    \n",
    "# Updating the states\n",
    "Updated_TimeStamp = TimeStamp.updateStateByKey(updateFunctionTimeStamp, initialRDD=state_timestamp_rdd)\n",
    "Updated_cumSums = cumSums.updateStateByKey(updateFunction, initialRDD=state_per_sensor_rdd)\n",
    "Updated_cumSums_squared = cumSums_squared.updateStateByKey(updateFunction, initialRDD=state_per_sensor_squared_rdd)\n",
    "Updated_covariance = Cum_Covariances.updateStateByKey(updateFunction, initialRDD=state_covariance_pairs_rdd)\n",
    "\n",
    "\n",
    "Updated_cumSums_flatten = Updated_cumSums.flatMap(lambda x : [(i, x[1].get(i)) for i in x[1]])\n",
    "Updated_cumSums_squared_flatten = Updated_cumSums_squared.flatMap(lambda x : [(i, x[1].get(i)) for i in x[1]])\n",
    "\n",
    "def get_correlation(x):\n",
    "    \"\"\"\n",
    "    Computes the correlation using the provided formula.\n",
    "\n",
    "    Args:\n",
    "        x: The input data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the pair identifier and the computed correlation.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        corr = (x[7]*x[2]-x[3]*x[5])/((math.sqrt(x[7]*x[4]-(x[3]**2)))*(math.sqrt(x[7]*x[6]-(x[5]**2))))\n",
    "    except ZeroDivisionError:\n",
    "        corr = -np.inf\n",
    "    return (x[0] + \"-\" + x[1], corr)\n",
    "\n",
    "# Joining the states to compute the correlation\n",
    "Correlation = Updated_covariance\\\n",
    "    .flatMap(lambda x: [(pair, x[1].get(pair)) for pair in x[1]])\\\n",
    "    .map(lambda x : (*x[0].split(\"-\"), x[1]))\\\n",
    "    .map(lambda x : (x[0], x))\\\n",
    "    .join(Updated_cumSums_flatten)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .join(Updated_cumSums_squared_flatten)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .map(lambda x : (x[1][1], x[1]))\\\n",
    "    .join(Updated_cumSums_flatten)\\\n",
    "    .map(lambda x : (x[0], (*x[1][0], x[1][1])))\\\n",
    "    .join(Updated_cumSums_squared_flatten)\\\n",
    "    .map(lambda x : (*x[1][0], x[1][1]))\\\n",
    "    .transformWith(lambda rdd1, rdd2: rdd1.cartesian(rdd2), Updated_TimeStamp)\\\n",
    "    .map(lambda x : (*x[0], x[1][1]))\\\n",
    "    .map(get_correlation)\n",
    "\n",
    "# Filtering to get the top 5 correlations\n",
    "Top_5 = Correlation\\\n",
    "    .transform(lambda rdd:rdd.ctx.parallelize(rdd.takeOrdered(5, lambda x: -x[1])))\n",
    "\n",
    "Updated_TimeStamp.pprint()\n",
    "Top_5.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:39:55\n",
      "-------------------------------------------\n",
      "('state_timestamp', 0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:39:55\n",
      "-------------------------------------------\n",
      "('CB1101-CJE181', -inf)\n",
      "('CEK18-CJE181', -inf)\n",
      "('CEV011-CJE181', -inf)\n",
      "('CB1143-CJE181', -inf)\n",
      "('CEK049-CJE181', -inf)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:00\n",
      "-------------------------------------------\n",
      "('state_timestamp', 0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:00\n",
      "-------------------------------------------\n",
      "('CB1101-CJE181', -inf)\n",
      "('CEK18-CJE181', -inf)\n",
      "('CEV011-CJE181', -inf)\n",
      "('CB1143-CJE181', -inf)\n",
      "('CEK049-CJE181', -inf)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:05\n",
      "-------------------------------------------\n",
      "('state_timestamp', 0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:05\n",
      "-------------------------------------------\n",
      "('CB1101-CJE181', -inf)\n",
      "('CEK18-CJE181', -inf)\n",
      "('CEV011-CJE181', -inf)\n",
      "('CB1143-CJE181', -inf)\n",
      "('CEK049-CJE181', -inf)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:10\n",
      "-------------------------------------------\n",
      "('state_timestamp', 2880)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:10\n",
      "-------------------------------------------\n",
      "('CB02411-CEK049', 0.7657636106436597)\n",
      "('CEK049-CJM90', 0.7500074623328697)\n",
      "('CB02411-CJM90', 0.7341503240487527)\n",
      "('CB2105-CJM90', 0.7071314753542198)\n",
      "('CB2105-CEK049', 0.6943233481656088)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:15\n",
      "-------------------------------------------\n",
      "('state_timestamp', 5760)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:15\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8895591661284046)\n",
      "('CB02411-CEK049', 0.8806025427740716)\n",
      "('CB02411-CJM90', 0.8799457993377243)\n",
      "('CB1143-CB2105', 0.7056776812543091)\n",
      "('CB1143-CJM90', 0.6933695688222551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:20\n",
      "-------------------------------------------\n",
      "('state_timestamp', 8640)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:20\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8199521335264696)\n",
      "('CB02411-CJM90', 0.7973627802301151)\n",
      "('CB02411-CEK049', 0.7969983218815763)\n",
      "('CB1143-CEK049', 0.6954347469601095)\n",
      "('CB1143-CJM90', 0.6299576745113024)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:25\n",
      "-------------------------------------------\n",
      "('state_timestamp', 8640)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:25\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8199521335264696)\n",
      "('CB02411-CJM90', 0.7973627802301151)\n",
      "('CB02411-CEK049', 0.7969983218815763)\n",
      "('CB1143-CEK049', 0.6954347469601095)\n",
      "('CB1143-CJM90', 0.6299576745113024)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:30\n",
      "-------------------------------------------\n",
      "('state_timestamp', 11520)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:30\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.819021704421524)\n",
      "('CB02411-CJM90', 0.7778404487961231)\n",
      "('CB02411-CEK049', 0.7759358889102512)\n",
      "('CB1143-CEK049', 0.7043333216786912)\n",
      "('CB1599-CLW239', 0.6986806136245722)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:35\n",
      "-------------------------------------------\n",
      "('state_timestamp', 14400)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:35\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8037096924896054)\n",
      "('CB02411-CEK049', 0.7514565342051365)\n",
      "('CB02411-CJM90', 0.748493056316119)\n",
      "('CB1143-CEK049', 0.7065300874988342)\n",
      "('CB1599-CLW239', 0.6979038112695906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:40\n",
      "-------------------------------------------\n",
      "('state_timestamp', 17280)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:40\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8091435174283385)\n",
      "('CB02411-CEK049', 0.7410423170281322)\n",
      "('CB02411-CJM90', 0.7376331123400591)\n",
      "('CB1599-CLW239', 0.6887482374975789)\n",
      "('CB2105-CEK049', 0.6774306410717481)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:45\n",
      "-------------------------------------------\n",
      "('state_timestamp', 17280)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:45\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8091435174283385)\n",
      "('CB02411-CEK049', 0.7410423170281322)\n",
      "('CB02411-CJM90', 0.7376331123400591)\n",
      "('CB1599-CLW239', 0.6887482374975789)\n",
      "('CB2105-CEK049', 0.6774306410717481)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:50\n",
      "-------------------------------------------\n",
      "('state_timestamp', 20160)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2023-05-14 18:40:50\n",
      "-------------------------------------------\n",
      "('CEK049-CJM90', 0.8078367575672482)\n",
      "('CB02411-CJM90', 0.7428091779203823)\n",
      "('CB1599-CLW239', 0.7118793540768974)\n",
      "('CB02411-CEK049', 0.7011036801865222)\n",
      "('CB1599-CB2105', 0.669829283816831)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
